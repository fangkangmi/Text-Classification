{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import spacy\n",
    "import pytextrank\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import networkx as nx\n",
    "import nltk\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy import spatial\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.summarization import summarize\n",
    "from transformers import LongformerTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4657, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Drowned Wednesday</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>Drowned Wednesday is the first Trustee among ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Lost Hero</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>As the book opens, Jason awakens on a school ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Eyes of the Overworld</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>Cugel is easily persuaded by the merchant Fia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Magic's Promise</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>The book opens with Herald-Mage Vanyel return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Taran Wanderer</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>Taran and Gurgi have returned to Caer Dallben...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                      title    genre  \\\n",
       "0      0          Drowned Wednesday  fantasy   \n",
       "1      1              The Lost Hero  fantasy   \n",
       "2      2  The Eyes of the Overworld  fantasy   \n",
       "3      3            Magic's Promise  fantasy   \n",
       "4      4             Taran Wanderer  fantasy   \n",
       "\n",
       "                                             summary  \n",
       "0   Drowned Wednesday is the first Trustee among ...  \n",
       "1   As the book opens, Jason awakens on a school ...  \n",
       "2   Cugel is easily persuaded by the merchant Fia...  \n",
       "3   The book opens with Herald-Mage Vanyel return...  \n",
       "4   Taran and Gurgi have returned to Caer Dallben...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from a CSV file and print its shape and first few rows\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"index\" column from the DataFrame\n",
    "data.drop(columns = [\"index\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num labels:  10\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique labels in the 'genre' column\n",
    "num_labels = len(data.genre.unique())\n",
    "print(\"num labels: \", num_labels)\n",
    "# Create mappings between genre labels and their corresponding IDs\n",
    "genre2id = {genre: i for i, genre in enumerate(data.genre.unique())}\n",
    "id2genre = {i: genre for i, genre in enumerate(data.genre.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Clean the input text by removing unwanted characters, links, numbers, etc.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    text = re.sub('[^a-zA-Z0-9\\.\\,\\?\\!]', ' ', str(text).lower()) # remove all except lowercase, uppercase, digits, punctuation\n",
    "    text = re.sub('\\[.*?\\]', '', text) # remove any text in square brackets\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # remove any links present \n",
    "    text = re.sub('\\n', ' ', text) # remove the next line character\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) # remove the words contaitning numbers\n",
    "    text = re.sub('\\s+', ' ', text) # remove extra spaces\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drowned Wednesday</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>drowned wednesday is the first trustee among ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Lost Hero</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>as the book opens, jason awakens on a school ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Eyes of the Overworld</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>cugel is easily persuaded by the merchant fia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Magic's Promise</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>the book opens with herald mage vanyel return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taran Wanderer</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>taran and gurgi have returned to caer dallben...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title    genre  \\\n",
       "0          Drowned Wednesday  fantasy   \n",
       "1              The Lost Hero  fantasy   \n",
       "2  The Eyes of the Overworld  fantasy   \n",
       "3            Magic's Promise  fantasy   \n",
       "4             Taran Wanderer  fantasy   \n",
       "\n",
       "                                             summary  \n",
       "0   drowned wednesday is the first trustee among ...  \n",
       "1   as the book opens, jason awakens on a school ...  \n",
       "2   cugel is easily persuaded by the merchant fia...  \n",
       "3   the book opens with herald mage vanyel return...  \n",
       "4   taran and gurgi have returned to caer dallben...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"summary\"] = data[\"summary\"].apply(clean_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thriller      1023\n",
       "fantasy        876\n",
       "science        647\n",
       "history        600\n",
       "horror         600\n",
       "crime          500\n",
       "romance        111\n",
       "psychology     100\n",
       "sports         100\n",
       "travel         100\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cugel is easily persuaded by the merchant fianosther to attempt the burglary of the manse of iucounu the laughing magician. trapped and caught, he agrees that in exchange for his freedom he will undertake the recovery of a small hemisphere of violet glass, an eye of the overworld, to match one already in the wizard s possession. a small sentient alien entity of barbs and hooks, named firx, is attached to his liver to encourage his unremitting loyalty, zeal and singleness of purpose, and iucounu uses a spell to transport cugel via flying demon to the remote land of cutz. there, cugel finds two villages, one occupied by wearers of the violet lenses, the other by peasants who work on behalf of the lens wearers, in hopes of being promoted to their ranks. the lenses cause their wearers to see, not their squalid surroundings, but the overworld, a vastly superior version of reality where a hut is a palace, gruel is a magnificent feast, etc. seeing the world through rose colored glasses on a grand scale. cugel gains an eye by trickery, and escapes from cutz. he then undertakes an arduous trek back to iucounu, cursing the magician the entire way this forms the principal part of the book. after many pitfalls, setbacks, and harrowing escapes, including the eviction of firx from his system, cugel returns to iucounu s manse, where he finds the wizard s volition has been captured by a twin to firx. cugel manages to extirpate the alien, subdue the magician, and enjoy the easy life in the manse, until he tries to banish iucounu and fianosther who himself has come to pilfer from cugel with the same spell that the magician had used on him. but cugel s tongue slips in uttering the incantation, and the flying demon seizes him instead, delivering him to the same spot as before. author michael shea wrote an authorized sequel, a quest for simbilis daw books, ny, . vance s own cugel sequel was published as cugel s saga in .\n",
      "\n",
      "-----------------------after TextRank---------------------\n",
      "\n",
      "trapped and caught, he agrees that in exchange for his freedom he will undertake the recovery of a small hemisphere of violet glass, an eye of the overworld, to match one already in the wizard s possession. a small sentient alien entity of barbs and hooks, named firx, is attached to his liver to encourage his unremitting loyalty, zeal and singleness of purpose, and iucounu uses a spell to transport cugel via flying demon to the remote land of cutz.\n"
     ]
    }
   ],
   "source": [
    "def text_rank_top_four(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # If the text has no sentences, return an empty string\n",
    "    if len(sentences) == 0:\n",
    "        return ''\n",
    "    \n",
    "    # If the text has less than four sentences, return the original text\n",
    "    if len(sentences) < 4:\n",
    "        return text\n",
    "    \n",
    "    # Use gensim's summarize function, which implements TextRank. We ask for a summary that is 20% of the original length.\n",
    "    summary = summarize(text, ratio=0.2)\n",
    "    \n",
    "    # Split the summary into sentences\n",
    "    summary_sentences = summary.split('\\n')\n",
    "    \n",
    "    # If the summary has no sentences, return an empty string\n",
    "    if len(summary_sentences) == 0:\n",
    "        return ''\n",
    "    \n",
    "    # Otherwise, return the first four sentences of the summary\n",
    "    return ' '.join(summary_sentences[:4])\n",
    "\n",
    "\n",
    "i=2\n",
    "print(data[\"summary\"][i])\n",
    "print(\"\\n-----------------------after TextRank---------------------\\n\")\n",
    "print(text_rank_top_four(data[\"summary\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we keep the first four sentences for each summary.\n",
    "\n",
    "try:\n",
    "    data[\"summary\"] = data[\"summary\"].apply(text_rank_top_four)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while applying text_rank_top_four:\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now all the data ready, need to train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(df, train_size=0.7, val_size=0.2, test_size=0.1, random_state=seed_val):\n",
    "    # Split the data into train and remaining data\n",
    "    train_data, remaining_data = train_test_split(df, train_size=train_size, random_state=random_state)\n",
    "\n",
    "    # Calculate the remaining size after the train split\n",
    "    remaining_size = val_size + test_size\n",
    "\n",
    "    # Split the remaining data into validation and test sets\n",
    "    val_data, test_data = train_test_split(remaining_data, test_size=remaining_size, random_state=random_state)\n",
    "\n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>the book begins when leaf is visiting arthur a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>annabeth, seeking percy, was told in a vision ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>trapped and caught, he agrees that in exchange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>the book opens with herald mage vanyel returni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>taran and gurgi have returned to caer dallben ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     genre                                            summary\n",
       "0  fantasy  the book begins when leaf is visiting arthur a...\n",
       "1  fantasy  annabeth, seeking percy, was told in a vision ...\n",
       "2  fantasy  trapped and caught, he agrees that in exchange...\n",
       "3  fantasy  the book opens with herald mage vanyel returni...\n",
       "4  fantasy  taran and gurgi have returned to caer dallben ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We drop the data title, and only train it based on summary:\n",
    "data = data.drop(\"title\",axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First step is to tokenize.\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "# Tokenize the text\n",
    "def tokenize_text(df, max_length=512):\n",
    "    # Tokenize the 'summary' column of the DataFrame using the tokenizer's batch_encode_plus method\n",
    "    # df['summary'].tolist() converts the 'summary' column into a list of strings\n",
    "    # max_length specifies the maximum length of the tokenized sequences\n",
    "    # padding='max_length' pads the tokenized sequences to have a length of max_length\n",
    "    # truncation=True truncates the tokenized sequences if they exceed the max_length\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        df['summary'].tolist(), \n",
    "        max_length=max_length, \n",
    "        padding='max_length', \n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "tokenized_texts = tokenize_text(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then prepare the dataset and dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "\n",
    "#convert genre labels to numerical values. \n",
    "le = LabelEncoder()\n",
    "data['genre'] = le.fit_transform(data['genre'])\n",
    "\n",
    "# Splitting the dataset into train and remaining (val + test)\n",
    "train_data, remaining_data = train_test_split(data, test_size=0.3, stratify=data['genre'], random_state=seed_val)\n",
    "\n",
    "# Splitting the remaining_data into validation and test\n",
    "val_data, test_data = train_test_split(remaining_data, test_size=0.333, stratify=remaining_data['genre'], random_state=seed_val)\n",
    "\n",
    "\n",
    "class BookDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = BookDataset(tokenized_texts, data['genre'].values)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the model and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model:\n",
    "\n",
    "from transformers import LongformerForSequenceClassification\n",
    "\n",
    "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', num_labels=len(data['genre'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to('cuda')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    for batch in dataloader:\n",
    "        # Move batch tensors to the same device as the model\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the model parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the new data: Just as you did with the training data, \n",
    "#you need to tokenize and format the new data so it's suitable for the model. \n",
    "#If you have a new book summary, you can prepare it like this:\n",
    "\n",
    "\n",
    "text = \"This is a new book summary...\"\n",
    "inputs = tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "inputs = inputs.to('cuda')  # move inputs to GPU\n",
    "\n",
    "\n",
    "#Make a prediction: Pass the prepared data to the model to get the predicted genre.\n",
    "outputs = model(**inputs)\n",
    "\n",
    "#Interpret the prediction: The model will output the logits for each genre. \n",
    "#You can convert these logits to probabilities using the softmax function, \n",
    "#and then get the genre with the highest probability.\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "predicted_genre = probabilities.argmax().item()\n",
    "\n",
    "#Remember that predicted_genre will be an integer representing the predicted genre. \n",
    "#If you've used a LabelEncoder to encode your genres, you can get the actual genre label like this:\n",
    "predicted_genre_label = le.inverse_transform([predicted_genre])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all the pretrained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"LongFormer_parameters\")\n",
    "\n",
    "#Load model by using below:\n",
    "#model = LongformerForSequenceClassification.from_pretrained(\"LongFormer_parameters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we could evaluate by using Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect the model's predictions and the true labels: \n",
    "from torch.nn.functional import softmax\n",
    "import numpy as np\n",
    "\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs, labels = batch['input_ids'].to('cuda'), batch['labels'].to('cuda')\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        probabilities = softmax(outputs.logits, dim=-1)\n",
    "        predictions = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_true.extend(labels.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the confusion matrix:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(all_true, all_preds)\n",
    "\n",
    "#Plot it\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Initialize list to hold preprocessed sentences\n",
    "    preprocessed_sentences = []\n",
    "\n",
    "    # Iterate over each sentence\n",
    "    for sentence in sentences:\n",
    "        # Make lower case\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # Tokenize into words\n",
    "        tokens = word_tokenize(sentence)\n",
    "        \n",
    "        # Remove stopwords and lemmatize the words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        \n",
    "        # Append to the list of preprocessed sentences\n",
    "        preprocessed_sentences.append(tokens)\n",
    "\n",
    "    return sentences,preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m top_sentences\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Test the function\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43msummarize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msummary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m, in \u001b[0;36msummarize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize_text\u001b[39m(text):\n\u001b[0;32m----> 2\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m(text)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m     sentence_tokens \u001b[38;5;241m=\u001b[39m preprocess_text(text)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Calculating word embedding\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess_text' is not defined"
     ]
    }
   ],
   "source": [
    "def summarize_text(text):\n",
    "    sentences = preprocess_text(text)[0]\n",
    "    sentence_tokens = preprocess_text(text)[1]\n",
    "    \n",
    "    # Calculating word embedding\n",
    "    w2v = Word2Vec(sentence_tokens, min_count=1)\n",
    "    sentence_embeddings = [[w2v.wv[word][0] for word in words] for words in sentence_tokens]\n",
    "\n",
    "    # Finding the maximum length of a sentence in the text\n",
    "    max_len = max(len(tokens) for tokens in sentence_tokens)\n",
    "\n",
    "    # Padding sentence embeddings with zeros to match the maximum sentence length\n",
    "    sentence_embeddings = [np.pad(embedding, (0, max_len - len(embedding)), 'constant') for embedding in sentence_embeddings]\n",
    "\n",
    "    # Calculating the similarity matrix\n",
    "    similarity_matrix = np.zeros([len(sentence_tokens), len(sentence_tokens)])\n",
    "    for i, row_embedding in enumerate(sentence_embeddings):\n",
    "        for j, column_embedding in enumerate(sentence_embeddings):\n",
    "            similarity_matrix[i][j] = 1 - spatial.distance.cosine(row_embedding, column_embedding)\n",
    "\n",
    "    # Implementing PageRank\n",
    "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(nx_graph,max_iter=5000)\n",
    "\n",
    "    # Sorting sentences\n",
    "    top_sentences_with_scores = sorted([(sentence, scores[index]) for index, sentence in enumerate(sentences)], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get top 4 sentences (maintaining the original order)\n",
    "    top_sentences = [sentence for sentence, score in top_sentences_with_scores[:4]]\n",
    "    top_sentences.sort(key = lambda x: sentences.index(x))\n",
    "\n",
    "    # Joining sentences\n",
    "    summary = ' '.join(top_sentences)\n",
    "\n",
    "    return top_sentences\n",
    "\n",
    "        \n",
    "# Test the function\n",
    "summarize_text(data['summary'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenize(data['summary'][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in row 262: you must first build vocabulary before training the model\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    try:\n",
    "        # Apply the summarize_text function and store the result in the new column\n",
    "        data.loc[i, 'summary'] = summarize_text(data.loc[i, 'summary'])\n",
    "    except Exception as e:\n",
    "        # If an error occurs, print the error message and skip this row\n",
    "        print(f\"Error in row {i}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences):\n",
    "    # Lowercase and remove punctuation\n",
    "    sentences_clean = [re.sub(r'[^\\w\\s]', '', sentence.lower()) for sentence in sentences]\n",
    "\n",
    "    # Load list of stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Tokenize sentences and remove stopwords\n",
    "    sentence_tokens = [[word for word in sentence.split(' ') if word not in stop_words] for sentence in sentences_clean]\n",
    "\n",
    "    return sentence_tokens\n",
    "data['summary'] = data['summary'].apply(preprocess_sentences)\n",
    "print(data['summary'][0])\n",
    "\n",
    "#Tokenizing every paragraph into a list of sentences\n",
    "data['summary'] = data['summary'].apply(sent_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' drowned wednesday is the first trustee among the morrow days who is on arthur s side and wishes the will to be fulfilled. she appears as a leviathan whale and suffers from gluttony. the book begins when leaf is visiting arthur and they are discussing the invitation that drowned wednesday sent him. arthur had been admitted to hospital because of the damage done to his leg when he attempted to enter tuesday s treasure tower. suddenly, the hospital room becomes flooded with water as the two are transported to the border sea of the house. leaf is snatched away by a large ship with green sails, known as the flying mantis, while arthur remains in his bed. when the medallion given him by the immortal called the mariner apparently fails to summon help, arthur is without hope. eventually, a buoy marking the pirate elishar feverfew s treasure floats toward him. as soon as arthur opens it, his hand is marked with a bloody red colour. arthur now has the red hand, by which feverfew marks whoever has found his treasure, so that he can identify them later. not long after, a scavenging ship called the moth rescues arthur. on board, arthur going by the name of arth is introduced to sunscorch, the first mate, and to captain catapillow. their journey brings them through the line of storms and into the border sea, where they are later pursued by feverfew s ghostly ship, the shiver. the damage inflicted on the moth is serious therefore sunscorch commands an upper house sorcerer, dr. scamandros, to open a transfer portal to elsewhere in the secondary realms. scamandros claims that arthur is carrying something that interfered with his magic, and tells sunscorch to throw him overboard. as a last resort, arthur shows them the mariner s medallion, which stops scamandros saying that they must get rid of arthur. after going through the transfer portal with arthur s help , the ship is grounded on a beach. when arthur wished to learn what happened to leaf, dr. scamandros applies his sorcery to make it possible. she is revealed to be aboard the ship flying mantis. arthur joins catapillow for supper, later to reveal his identity. at first, propaganda issued by dame primus arthur s steward makes them skeptical of this, but they eventually become convinced. a few days later, wednesday s dawn takes arthur to meet wednesday for her luncheon of seventeen removes . as they approach, wednesday shrinks into her human form to meet arthur. during their lunch, wednesday tells arthur that after part of the will has been released, she will surrender the third key to arthur. arthur is then taken by wednesday s dawn to a place called the triangle in search of his friend leaf. he learns that leaf has been forced to work on the mantis, but is otherwise intact. arthur later makes a deal with the raised rats, a group of anthropomorphic rats brought to the house by the piper, to take him to feverfew s hideout, which they believe is inside a miniature world located within drowned wednesday s stomach. on the raised rats ship, arthur opens a gift from dr. scamandros, which proves to be a golden transfer watch. with this, he communicates with and rescues dr. scamandros. he then uses a scrying mirror dr. scamandros gave him to watch leaf. a rat watches him during the scry, and later saves him from a battle with feverfew. later, the rats bring arthur onto their submarine, where he meets with suzy turquiose blue. in contrast to suzy s former cockney attitude, she has assumed a more ladylike and proper demeanor on the orders of dame primus. only when they are no longer on the border sea, but under it, does she resume her customary ways of speech and dress. they are, with navigational difficulty, able to enter the stomach of lady wednesday and the worldlet therein. there, arthur and suzy, disguised as rats, find escaped slaves, professed followers of the carp. these exiles take them to the carp, who is the third part of the will. they are halted in their attempt to escape by feverfew, who proposes that each of them will try to kill the other by means of one strike only. arthur fails his first try, then dodges feverfew and severs his head. leaf, who is feverfew s prisoner, then kicks it into a mud puddle containing nothing, which consumes it. upon his death, the worldlet begins to collapse. via the moth, arthur and all his friends with the exception of the reluctant catapillow are able to escape. lady wednesday recovers from her gluttony, then dies as a result of being poisoned by the worldlet, which had opened a void to nothing. arthur, now duke of the border sea, appoints sunscorch as his noon and scamandros as his dusk. fr mercredi sous les flots th '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['summary'] = data['summary'].apply(gensim_summarizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are all previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['summary'][0]\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "doc = nlp(text)\n",
    "# examine the top-ranked phrases in the document\n",
    "for phrase in doc._.phrases:\n",
    "    print(phrase.text)\n",
    "    print(phrase.rank, phrase.count)\n",
    "    print(phrase.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasy_summaries = data[data[\"genre\"] == id2genre[0]]\n",
    "science_summaries = data[data[\"genre\"] == id2genre[1]]\n",
    "crime_summaries = data[data[\"genre\"] == id2genre[2]]\n",
    "history_summaries = data[data[\"genre\"] == id2genre[3]]\n",
    "horror_summaries = data[data[\"genre\"] == id2genre[4]]\n",
    "thriller_summaries = data[data[\"genre\"] == id2genre[5]]\n",
    "psychology_summaries = data[data[\"genre\"] == id2genre[6]]\n",
    "romance_summaries = data[data[\"genre\"] == id2genre[7]]\n",
    "sports_summaries = data[data[\"genre\"] == id2genre[8]]\n",
    "travel_summaries = data[data[\"genre\"] == id2genre[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the fantasy_summaries dataset\n",
    "fantasy_downsample = resample(fantasy_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=300,\n",
    "                              random_state=42)\n",
    "\n",
    "science_downsample = resample(science_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=300,\n",
    "                              random_state=42)\n",
    "\n",
    "crime_downsample = resample(crime_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=300,\n",
    "                              random_state=42)\n",
    "\n",
    "history_downsample = resample(history_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=300,\n",
    "                              random_state=42)\n",
    "\n",
    "horror_downsample = resample(horror_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=300,\n",
    "                              random_state=42)\n",
    "\n",
    "thriller_downsample = resample(thriller_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=300,\n",
    "                              random_state=42)\n",
    "\n",
    "psychology_downsample = resample(psychology_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=80,\n",
    "                              random_state=42)\n",
    "\n",
    "romance_downsample = resample(romance_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=80,\n",
    "                              random_state=42)\n",
    "\n",
    "sports_downsample = resample(sports_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=80,\n",
    "                              random_state=42)\n",
    "\n",
    "travel_downsample = resample(travel_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=80,\n",
    "                              random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate it VERTICALLY\n",
    "train = pd.concat([fantasy_downsample, science_downsample, crime_downsample, history_downsample, horror_downsample, thriller_downsample, psychology_downsample, romance_downsample, sports_downsample, travel_downsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "fantasy       300\n",
       "science       300\n",
       "crime         300\n",
       "history       300\n",
       "horror        300\n",
       "thriller      300\n",
       "psychology     80\n",
       "romance        80\n",
       "sports         80\n",
       "travel         80\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data.loc[~data.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "thriller      723\n",
       "fantasy       576\n",
       "science       347\n",
       "history       300\n",
       "horror        300\n",
       "crime         200\n",
       "romance        31\n",
       "psychology     20\n",
       "sports         20\n",
       "travel         20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "import torch\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsum",
   "language": "python",
   "name": "textsum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
