{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4657, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Drowned Wednesday</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>Drowned Wednesday is the first Trustee among ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Lost Hero</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>As the book opens, Jason awakens on a school ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Eyes of the Overworld</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>Cugel is easily persuaded by the merchant Fia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Magic's Promise</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>The book opens with Herald-Mage Vanyel return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Taran Wanderer</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>Taran and Gurgi have returned to Caer Dallben...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                      title    genre  \\\n",
       "0      0          Drowned Wednesday  fantasy   \n",
       "1      1              The Lost Hero  fantasy   \n",
       "2      2  The Eyes of the Overworld  fantasy   \n",
       "3      3            Magic's Promise  fantasy   \n",
       "4      4             Taran Wanderer  fantasy   \n",
       "\n",
       "                                             summary  \n",
       "0   Drowned Wednesday is the first Trustee among ...  \n",
       "1   As the book opens, Jason awakens on a school ...  \n",
       "2   Cugel is easily persuaded by the merchant Fia...  \n",
       "3   The book opens with Herald-Mage Vanyel return...  \n",
       "4   Taran and Gurgi have returned to Caer Dallben...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = [\"index\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num labels:  10\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(data.genre.unique())\n",
    "print(\"num labels: \", num_labels)\n",
    "genre2id = {genre: i for i, genre in enumerate(data.genre.unique())}\n",
    "id2genre = {i: genre for i, genre in enumerate(data.genre.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub('[^a-zA-Z0-9\\.\\,\\?\\!]', ' ', str(text).lower()) # remove all except lowercase, uppercase, digits, punctuation\n",
    "    text = re.sub('\\[.*?\\]', '', text) # remove any text in square brackets\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # remove any links present \n",
    "    text = re.sub('\\n', ' ', text) # remove the next line character\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) # remove the words contaitning numbers\n",
    "    text = re.sub('\\s+', ' ', text) # remove extra spaces\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drowned Wednesday</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>drowned wednesday is the first trustee among ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Lost Hero</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>as the book opens, jason awakens on a school ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Eyes of the Overworld</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>cugel is easily persuaded by the merchant fia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Magic's Promise</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>the book opens with herald mage vanyel return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taran Wanderer</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>taran and gurgi have returned to caer dallben...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title    genre  \\\n",
       "0          Drowned Wednesday  fantasy   \n",
       "1              The Lost Hero  fantasy   \n",
       "2  The Eyes of the Overworld  fantasy   \n",
       "3            Magic's Promise  fantasy   \n",
       "4             Taran Wanderer  fantasy   \n",
       "\n",
       "                                             summary  \n",
       "0   drowned wednesday is the first trustee among ...  \n",
       "1   as the book opens, jason awakens on a school ...  \n",
       "2   cugel is easily persuaded by the merchant fia...  \n",
       "3   the book opens with herald mage vanyel return...  \n",
       "4   taran and gurgi have returned to caer dallben...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"summary\"] = data[\"summary\"].apply(clean_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"genre_id\"] = data[\"genre\"].apply(lambda a: genre2id[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thriller      1023\n",
       "fantasy        876\n",
       "science        647\n",
       "history        600\n",
       "horror         600\n",
       "crime          500\n",
       "romance        111\n",
       "psychology     100\n",
       "sports         100\n",
       "travel         100\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasy_summaries = data[data[\"genre_id\"] == 0]\n",
    "science_summaries = data[data[\"genre_id\"] == 1]\n",
    "crime_summaries = data[data[\"genre_id\"] == 2]\n",
    "history_summaries = data[data[\"genre_id\"] == 3]\n",
    "horror_summaries = data[data[\"genre_id\"] == 4]\n",
    "thriller_summaries = data[data[\"genre_id\"] == 5]\n",
    "psychology_summaries = data[data[\"genre_id\"] == 6]\n",
    "romance_summaries = data[data[\"genre_id\"] == 7]\n",
    "sports_summaries = data[data[\"genre_id\"] == 8]\n",
    "travel_summaries = data[data[\"genre_id\"] == 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "fantasy_downsample = resample(fantasy_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=300,\n",
    "                              random_state=42)\n",
    "\n",
    "science_downsample = resample(science_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=300,\n",
    "                              random_state=42)\n",
    "\n",
    "crime_downsample = resample(crime_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=300,\n",
    "                              random_state=42)\n",
    "\n",
    "history_downsample = resample(history_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=300,\n",
    "                              random_state=42)\n",
    "\n",
    "horror_downsample = resample(horror_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=300,\n",
    "                              random_state=42)\n",
    "\n",
    "thriller_downsample = resample(thriller_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=300,\n",
    "                              random_state=42)\n",
    "\n",
    "psychology_downsample = resample(psychology_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=80,\n",
    "                              random_state=42)\n",
    "\n",
    "romance_downsample = resample(romance_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=80,\n",
    "                              random_state=42)\n",
    "\n",
    "sports_downsample = resample(sports_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=80,\n",
    "                              random_state=42)\n",
    "\n",
    "travel_downsample = resample(travel_summaries,\n",
    "                              replace=False,\n",
    "                              n_samples=80,\n",
    "                              random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([fantasy_downsample, science_downsample, crime_downsample, history_downsample, horror_downsample, thriller_downsample, psychology_downsample, romance_downsample, sports_downsample, travel_downsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fantasy       300\n",
       "science       300\n",
       "crime         300\n",
       "history       300\n",
       "horror        300\n",
       "thriller      300\n",
       "psychology     80\n",
       "romance        80\n",
       "sports         80\n",
       "travel         80\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data.loc[~data.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thriller      723\n",
       "fantasy       576\n",
       "science       347\n",
       "history       300\n",
       "horror        300\n",
       "crime         200\n",
       "romance        31\n",
       "psychology     20\n",
       "sports         20\n",
       "travel         20\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>summary</th>\n",
       "      <th>genre_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>The Forest House</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>in the early days of the conquest, when the r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4436</th>\n",
       "      <td>Heartless</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>long before she was the terror of wonderland t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>A Hat Full of Sky</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>we see you. now we are you. no real witch woul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4473</th>\n",
       "      <td>Going Postal</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>arch swindler moist van lipwig never believed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Tithe : A Modern Faerie Tale</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>tithe follows the story of sixteen year old a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title    genre  \\\n",
       "365               The Forest House  fantasy   \n",
       "4436                     Heartless  fantasy   \n",
       "4609             A Hat Full of Sky  fantasy   \n",
       "4473                  Going Postal  fantasy   \n",
       "213   Tithe : A Modern Faerie Tale  fantasy   \n",
       "\n",
       "                                                summary  genre_id  \n",
       "365    in the early days of the conquest, when the r...         0  \n",
       "4436  long before she was the terror of wonderland t...         0  \n",
       "4609  we see you. now we are you. no real witch woul...         0  \n",
       "4473  arch swindler moist van lipwig never believed ...         0  \n",
       "213    tithe follows the story of sixteen year old a...         0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, data has been cleaned. Next step is to tokenize, train, and test.\n",
    "The first step is to use TextRank from Gensim to rank the first few sentences, and then we will train on BERT and RoBERTA to see its' performance.\n",
    "\n",
    "# Text Rank by Gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.summarization import summarize\n",
    "def text_rank(text,num):\n",
    "    \"\"\"\n",
    "    Apply TextRank algorithm to summarize the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be summarized.\n",
    "        num (int): The desired number of sentences in the summary.\n",
    "\n",
    "    Returns:\n",
    "        str: The summarized text.\n",
    "\n",
    "    \"\"\"\n",
    "    # Split the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # If the text has no sentences, return an empty string\n",
    "    if len(sentences) == 0:\n",
    "        return ''\n",
    "    \n",
    "    # If the text has less than four sentences, return the original text\n",
    "    if len(sentences) < num:\n",
    "        return text\n",
    "    \n",
    "    # Use gensim's summarize function, which implements TextRank. We ask for a summary that is 20% of the original length.\n",
    "    summary = summarize(text, ratio=0.3)\n",
    "    \n",
    "    # Split the summary into sentences\n",
    "    summary_sentences = summary.split('\\n')\n",
    "    \n",
    "    # If the summary has no sentences, return an empty string\n",
    "    if len(summary_sentences) == 0:\n",
    "        return ''\n",
    "    \n",
    "    # Otherwise, return the first four sentences of the summary\n",
    "    return ' '.join(summary_sentences[:num])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------before TextRank---------------------\n",
      "\n",
      " the blieder drive, a faster than light drive system, has permitted the population of earth to colonize the galaxy. each planet has become the home for a particular social group. four hundred years after the diaspora, a spaceship from earth visits three of the planets, the first steps to unifying the galaxy under a new empire. things do not go entirely as hoped, as the incompetent military authoritarians of the ship encounter three very different societies. the first planet was a penal colony it is now many independent kleptocratic despotisms preying on each other. the second planet, hygeia, is populated by health and fitness fanatic nudists. the third planet, kassim, was colonized by a religious group, but when the ship arrives, they can t find any human life, only empty villages overgrown by jungle. they decide not to land on the planet, because the captain fears that the colonists could have been killed by a disease and he doesn t want to endanger the crew. the final planet, , has developed an unusual social system. the population call themselves gands after gandhi and practise a form of classless, philosophically anarchic libertarianism, based on passive resistance freedom i won t! and myob! and a moneyless gift economy based on barter and favor exchange, using obs obligations . to perform a service for somebody lays an ob on them they can then kill the ob by returning the favor.\n",
      "\n",
      "-----------------------Progressing---------------------\n",
      "\n",
      "\n",
      "-----------------------after TextRank---------------------\n",
      "\n",
      "the blieder drive, a faster than light drive system, has permitted the population of earth to colonize the galaxy. each planet has become the home for a particular social group. the third planet, kassim, was colonized by a religious group, but when the ship arrives, they can t find any human life, only empty villages overgrown by jungle.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Now we keep the first few sentences for each summary.\n",
    "\n",
    "print(\"\\n-----------------------before TextRank---------------------\\n\")\n",
    "print(train[\"summary\"][575])\n",
    "\n",
    "print(\"\\n-----------------------Progressing---------------------\\n\")\n",
    "try:\n",
    "    train[\"summary\"] = train[\"summary\"].apply(lambda x: text_rank(x, num=4))\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while applying text_rank:\", str(e))\n",
    "\n",
    "    \n",
    "print(\"\\n-----------------------after TextRank---------------------\\n\")\n",
    "print(train[\"summary\"][575])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForMaskedLM\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create a tokenizer\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_text\u001b[39m(df, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Tokenize the 'summary' column of the DataFrame using the tokenizer's batch_encode_plus method\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# df['summary'].tolist() converts the 'summary' column into a list of strings\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# max_length specifies the maximum length of the tokenized sequences\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# padding='max_length' pads the tokenized sequences to have a length of max_length\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# truncation=True truncates the tokenized sequences if they exceed the max_length\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "def tokenize_text(df, max_length=512):\n",
    "    # Tokenize the 'summary' column of the DataFrame using the tokenizer's batch_encode_plus method\n",
    "    # df['summary'].tolist() converts the 'summary' column into a list of strings\n",
    "    # max_length specifies the maximum length of the tokenized sequences\n",
    "    # padding='max_length' pads the tokenized sequences to have a length of max_length\n",
    "    # truncation=True truncates the tokenized sequences if they exceed the max_length\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        df['summary'].tolist(), \n",
    "        max_length=max_length, \n",
    "        padding='max_length', \n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "tokenized_train_texts = tokenize_text(train)\n",
    "tokenized_test_texts = tokenize_text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then prepare the dataset and dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class BookDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = BookDataset(tokenized_train_texts, train['genre_id'].values)\n",
    "test_dataset = BookDataset(tokenized_test_texts, test['genre_id'].values)\n",
    "\n",
    "# Create the dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the model and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(train['genre_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to('cuda')\n",
    "\n",
    "# Initialize a list to store the loss values\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    for batch in train_dataloader:\n",
    "        # Move batch tensors to the same device as the model\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the model parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Append the loss value to the list\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Plot the loss values after each epoch\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over Iterations')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive\n",
    "\n",
    "'''\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your dataframes are named 'train' and 'test'\n",
    "train_df = train\n",
    "test_df = test\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Preprocessing\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create Datasets\n",
    "max_len = 64\n",
    "batch_size = 4\n",
    "train_dataset = TextClassificationDataset(\n",
    "    texts=train_df.summary.to_numpy(),\n",
    "    labels=train_df.genre_id.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    ")\n",
    "test_dataset = TextClassificationDataset(\n",
    "    texts=test_df.summary.to_numpy(),\n",
    "    labels=test_df.genre_id.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    ")\n",
    "\n",
    "# Create Dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_labels = train_df['genre_id'].nunique()\n",
    "\n",
    "# Model and Optimizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "losses = []\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(losses)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# Define the loss function\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to('cuda')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        total_examples = 0\n",
    "        for batch in test_dataloader:\n",
    "            batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            labels = batch['labels']\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "            total_examples += len(labels)\n",
    "\n",
    "        accuracy = total_correct / total_examples\n",
    "        print(f\"Epoch {epoch + 1} - Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, summary, targets, tokenizer, max_len):\n",
    "        self.summary = summary\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.summary)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        summary = str(self.summary[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          summary,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          padding='max_length',\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "          truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "          'summary_text': summary,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    \n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = BertDataset(\n",
    "        summary=df.summary.to_numpy(),\n",
    "        targets=df.genre_id.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsum",
   "language": "python",
   "name": "textsum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
