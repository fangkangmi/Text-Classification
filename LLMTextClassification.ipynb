{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "054c1c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import spacy\n",
    "import pytextrank\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer,RobertaTokenizer, LongformerTokenizer,  BigBirdTokenizerFast\n",
    "from transformers import BertForSequenceClassification, RobertaForSequenceClassification, LongformerForSequenceClassification, BigBirdForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cbad0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "# If GPU not available, training will cost SEVERAL DAYS, not recommended running on CPU\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU insteadp(not recommended).')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f780e9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Load the model, if no model to load, initialize all the model and tokenizer for further use.\n",
    "#There will be several warning message pop out when initializing the model, \n",
    "# it is because of the model haven't been trained, it could be ignored.\n",
    "\n",
    "checkpoint_path = \"Model.pth\"  # Change to your preferred location\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    losses = checkpoint['losses']\n",
    "    print(\"Loaded checkpoint.\")\n",
    "else:\n",
    "    tokenizer_options = {\n",
    "        \"bert\": BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "        \"roberta\": RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "        \"longformer\": LongformerTokenizer.from_pretrained('allenai/longformer-base-4096'),\n",
    "        \"bigbird\":  BigBirdTokenizerFast.from_pretrained('l-yohai/bigbird-roberta-base-mnli')\n",
    "    }\n",
    "\n",
    "    model_options = {\n",
    "        \"bert\": BertForSequenceClassification.from_pretrained('bert-base-uncased'),\n",
    "        \"roberta\": RobertaForSequenceClassification.from_pretrained('roberta-base'),\n",
    "        \"longformer\": LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096'),\n",
    "        \"bigbird\": BigBirdForSequenceClassification.from_pretrained('l-yohai/bigbird-roberta-base-mnli')\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba8af0",
   "metadata": {},
   "source": [
    "# Set all the variable here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1cd0d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the input to how many tokens, default to 512 for BERT use.\n",
    "# Since LongFormer could accept 4096 tokens, we may could skip TextRank if LongFormer\n",
    "ENABLE_TEXT_RANK = False\n",
    "TEXT_RANK_LENGTH = 512\n",
    "\n",
    "MODEL = \"bert\" # Choose from \"bert\", \"roberta\", \"longformer\", \"bigbird\"\n",
    "seed_val = 42\n",
    "\n",
    "TRAIN_DATA_PATH = \"data/ds1_train.csv\"\n",
    "TEST_DATA_PATH = \"data/ds1_test.csv\"\n",
    "DATASET = \"ds1\" if \"ds1\" in TRAIN_DATA_PATH else \"ds2\"\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e8d9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the selected tokenizer and model\n",
    "tokenizer = tokenizer_options.get(MODEL)\n",
    "model = model_options.get(MODEL)\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7fedd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "test = pd.read_csv(TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d64f0922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2120, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>in the early days of the conquest, when the r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>long before she was the terror of wonderland t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>we see you. now we are you. no real witch woul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>arch swindler moist van lipwig never believed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>tithe follows the story of sixteen year old a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  label_id\n",
       "0  fantasy   in the early days of the conquest, when the r...         0\n",
       "1  fantasy  long before she was the terror of wonderland t...         0\n",
       "2  fantasy  we see you. now we are you. no real witch woul...         0\n",
       "3  fantasy  arch swindler moist van lipwig never believed ...         0\n",
       "4  fantasy   tithe follows the story of sixteen year old a...         0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e66c79",
   "metadata": {},
   "source": [
    "By now, data has been cleaned. Next step is to tokenize, train, and test.\n",
    "The first step is to use TextRank from Gensim to rank the first few sentences, and then we will train on BERT and RoBERTA to see its' performance.\n",
    "\n",
    "# Text Rank by Spacy:\n",
    "Note that this could be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6844a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "def summarize(input_text: str, max_tokens: int = 512):\n",
    "    # Load a SpaCy model. For example, the English model.\n",
    "\n",
    "\n",
    "    # Parse the document with SpaCy.\n",
    "    doc = nlp(input_text)\n",
    "\n",
    "    # If the text is less than 512 words, return the original text.\n",
    "    if len(doc) <= max_tokens:\n",
    "        return input_text\n",
    "\n",
    "    # Extract the phrases using TextRank.\n",
    "    phrases = \", \".join([p.text for p in doc._.phrases])\n",
    "\n",
    "    summarized_doc = []  # Initialize an empty list to store the summarized sentences\n",
    "\n",
    "    # Iterate over the sentences generated by textrank.summary and append them to the list\n",
    "    for sent in doc._.textrank.summary(limit_sentences=50):\n",
    "        summarized_doc.append(sent)\n",
    "\n",
    "\n",
    "    # Join the sentences to form the summary.\n",
    "    summary = \" \".join([span.text for span in summarized_doc])\n",
    "\n",
    "    # If the summary is longer than max_tokens, truncate it.\n",
    "    if len(summary.split()) > max_tokens:\n",
    "        summary = \" \".join(summary.split()[:max_tokens])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0364cbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextRank has been skipped\n"
     ]
    }
   ],
   "source": [
    "if (ENABLE_TEXT_RANK):\n",
    "    print(\"\\n-----------------------Progressing---------------------\\n\")\n",
    "    try:\n",
    "        train[\"text\"] = train[\"text\"].apply(summarize)\n",
    "        test[\"text\"] = test[\"text\"].apply(summarize)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while applying text_rank:\", str(e))\n",
    "    print(\"\\n-----------------------Done---------------------\\n\")\n",
    "else:\n",
    "    print('TextRank has been skipped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad29ad",
   "metadata": {},
   "source": [
    "# Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e858f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train.text.values\n",
    "train_labels = train.label_id.values\n",
    "\n",
    "test_text = test.text.values\n",
    "test_labels = test.label_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad9e0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "def tokenize_text(summary, max_length=512):\n",
    "    # Tokenize the 'summary' column of the DataFrame using the tokenizer's batch_encode_plus method\n",
    "    # df['summary'].tolist() converts the 'summary' column into a list of strings\n",
    "    # max_length specifies the maximum length of the tokenized sequences\n",
    "    # padding='max_length' pads the tokenized sequences to have a length of max_length\n",
    "    # truncation=True truncates the tokenized sequences if they exceed the max_length\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        summary.tolist(), \n",
    "        max_length=max_length, \n",
    "        padding='max_length', \n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "tokenized_train_texts = tokenize_text(train_summary)\n",
    "tokenized_test_texts = tokenize_text(test_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e410740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = TextClassificationDataset(tokenized_train_texts, train['label_id'].values)\n",
    "test_dataset = TextClassificationDataset(tokenized_test_texts, test['label_id'].values)\n",
    "\n",
    "# Create the dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e7b11",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dede58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.train()\n",
    "model.to('cuda')\n",
    "\n",
    "# Initialize a list to store the loss values\n",
    "losses = []\n",
    "\n",
    "checkpoint_path = f\"{MODEL}_{DATASET}_TextRank_{ENABLE_TEXT_RANK}.pth\" # Change to your preferred location\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "# Load from checkpoint if it exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    losses = checkpoint['losses']\n",
    "    print(\"Loaded checkpoint.\")\n",
    "\n",
    "# Training \n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    print(f\"Starting epoch {epoch}\")\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the model parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Append the loss value to the list\n",
    "        losses.append(loss.item())  \n",
    "\n",
    "    # Save checkpoint after each epoch\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': losses,\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Saved checkpoint for epoch {epoch}.\")\n",
    "\n",
    "    # Plot the loss values after each epoch\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over Iterations')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d1964",
   "metadata": {},
   "source": [
    "# Model now been trained, now evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a476b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If yo have a trained model just want to evaluate it:\n",
    "checkpoint_path = \"Model.pth\"  # Change to your preferred location\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    losses = checkpoint['losses']\n",
    "    print(\"Loaded checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to('cuda')\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Perform inference\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "    predicted_values = outputs.logits\n",
    "    predictions.extend(predicted_values)\n",
    "    true_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "# Convert logits to predictions\n",
    "predictions = [torch.argmax(item).item() for item in predictions]\n",
    "\n",
    "# Make sure that true_labels is a list\n",
    "assert isinstance(true_labels, list)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "  \n",
    "# Generate classification report\n",
    "report = classification_report(true_labels, predictions)\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save the report to a file\n",
    "with open(f'Classreport_{MODEL}_{DATASET}_TextRank_{ENABLE_TEXT_RANK}.txt', 'w') as file:\n",
    "    file.write(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsum",
   "language": "python",
   "name": "textsum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
