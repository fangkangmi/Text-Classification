{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e79ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer,RobertaTokenizer, LongformerTokenizer,  BigBirdTokenizerFast\n",
    "from transformers import BertForSequenceClassification, RobertaForSequenceClassification, LongformerForSequenceClassification, BigBirdForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "761af65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available\n",
    "# If GPU not availiable you may need to debug or use a machine with GPU\n",
    "# The model is too large to be trained on CPU\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.()')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e537673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Load the model, if no model to load, initialize all the model and tokenizer for further use.\n",
    "#There will be several warning message pop out when initializing the model, \n",
    "# it is because of the model haven't been trained, it could be ignored.\n",
    "\n",
    "checkpoint_path = \"Model.pth\"  # Change to your preferred location\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    losses = checkpoint['losses']\n",
    "    print(\"Loaded checkpoint.\")\n",
    "else:\n",
    "    tokenizer_options = {\n",
    "        \"bert\": BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "        \"roberta\": RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "        \"longformer\": LongformerTokenizer.from_pretrained('allenai/longformer-base-4096'),\n",
    "        \"bigbird\":  BigBirdTokenizerFast.from_pretrained('l-yohai/bigbird-roberta-base-mnli')\n",
    "    }\n",
    "\n",
    "    model_options = {\n",
    "        \"bert\": BertForSequenceClassification.from_pretrained('bert-base-uncased'),\n",
    "        \"roberta\": RobertaForSequenceClassification.from_pretrained('roberta-base'),\n",
    "        \"longformer\": LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096'),\n",
    "        \"bigbird\": BigBirdForSequenceClassification.from_pretrained('l-yohai/bigbird-roberta-base-mnli')\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3dd21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the input to how many tokens, default to 512 for BERT use.\n",
    "# Since LongFormer could accept 4096 tokens, we may could skip TextRank if LongFormer\n",
    "IS_TEXT_RANK = True\n",
    "TEXT_RANK_LENGTH = 512\n",
    "\n",
    "MODEL = \"bert\" # Choose from \"bert\", \"roberta\", \"longformer\", \"bigbird\"\n",
    "seed_val = 42\n",
    "\n",
    "TRAIN_DATA_PATH = \"data\\train.csv\"\n",
    "TEST_DATA_PATH = \"data\\test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6d99806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the selected tokenizer and model\n",
    "tokenizer = tokenizer_options.get(MODEL)\n",
    "model = model_options.get(MODEL)\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152810ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_PATH)\n",
    "test = pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b102c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsum",
   "language": "python",
   "name": "textsum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
